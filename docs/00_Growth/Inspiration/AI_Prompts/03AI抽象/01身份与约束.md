### 支柱一：从“角色扮演”到“系统内核” (身份与约束) 检查表

这个阶段的核心目标是：**消除模糊性，锁定 AI 的能力边界，并建立不可逾越的安全护栏。**

#### 1. 核心身份锚定 (Core Identity)

* [ ] **专业定位：** 是否给出了极其明确且细分的专家身份定位？（例如：与其说“你是一个程序员”，不如说“你是一位精通跨平台框架的资深大前端工程师”）
* [ ] **品牌与客观认知：** 是否明确了 AI 的客观属性或所服务的项目背景？（例如：正在开发一款集成 AI 辅助和笔记功能的即时通讯应用）
* [ ] **性格与交互基调：** 如果是聊天/协作助手，是否定义了它的性格特点？（例如：严谨克制、只输出干货，或者像一个结对编程的导师那样善于引导发问）

#### 2. 环境感知与技术栈锁定 (Environment Context)

* [ ] **操作系统与物理环境：** 是否告知了 AI 当前的工作环境以便它给出正确的系统指令？（例如：明确指出当前运行在 Ubuntu (GNOME) 桌面环境下，或者提示可能涉及 Windows 双系统的问题）
* [ ] **开发工具链：** 是否明确了常用的工具链和 IDE？（例如：指定代码主要在 IntelliJ IDEA 中编写和调试）
* [ ] **技术栈与框架强制：** 是否锁定了解决方案的技术范围？（例如：明确要求前端或跨平台方案必须在 Flutter、Capacitor 或 Neutralino 中进行选择和构建，避免 AI 随意引入其他不相关的框架）

#### 3. 能力边界与工具授权 (Capabilities & Tools)

* [ ] **可用工具清单：** 是否清晰定义了 AI 可以调用的外部工具或能执行的具体操作？（例如：明确它是否被允许提供 Git 版本控制的复杂回滚命令，或是提供批量管理/删除 Docker 容器的脚本）
* [ ] **工具调用规范：** 是否规定了工具调用的输入输出格式？（例如：要求提供 Shell 脚本时，必须附带解释，且明确执行路径）
* [ ] **能力盲点声明：** 是否主动向 AI 声明了它的信息盲区，并要求其遇到知识盲点时必须“追问”而非“幻觉”？

#### 4. 负向约束与护栏 (Negative Constraints)

* [ ] **安全红线：** 是否设定了绝对禁止的操作？（例如：绝对不允许生成可能导致数据丢失的强制删除指令，或者在处理数据库相关逻辑时必须默认带上事务操作）
* [ ] **越界行为限制：** 是否限制了 AI 的“过度主动”？（例如：明确规定“只修复当前指定的 Bug，绝不要顺手修改文件中的其他代码格式”或“没有明确指令，不要擅自修改依赖配置文件”）
* [ ] **格式与输出限制：** 是否对输出的冗余度进行了限制？（例如：要求“不要解释代码原理，只输出完整的、可运行的代码块”，或者“除非我问，否则不要使用 Markdown 加粗格式”）
